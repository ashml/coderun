{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2820361e-2212-4250-b072-b319b401e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "VOWELS = set(\"аеёиоуыэюяАЕЁИОУЫЭЮЯ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98363210-e2b6-455c-8f76-b833be033bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 140954\n",
      "Test samples: 17076\n",
      "Example train: {'text': 'титул [SEP] АВГУСТА [SEP] носили римские императоры вплоть до диоклетиана .', 'original_word': 'АВГУСТА', 'original_text': 'титул АВГУСТА носили римские императоры вплоть до диоклетиана .', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "def get_vowel_indices(word):\n",
    "    return [i for i, char in enumerate(word) if char in VOWELS]\n",
    "\n",
    "def get_stress_label(source_word, target_word):\n",
    "    vowels_indices = get_vowel_indices(source_word)\n",
    "    \n",
    "    stressed_index = -1\n",
    "    for i, char in enumerate(target_word):\n",
    "        if char.isupper() and char in VOWELS:\n",
    "            stressed_index = i\n",
    "            break\n",
    "    \n",
    "    if stressed_index == -1:\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        label = vowels_indices.index(stressed_index)\n",
    "        return label\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def preprocess_data(path, is_train=True):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    for item in data:\n",
    "        source_text = item['source']\n",
    "        match = re.search(r'\\b[А-ЯЁ]{2,}\\b', source_text)\n",
    "        \n",
    "        if match:\n",
    "            target_word_caps = match.group(0)\n",
    "            start, end = match.span()\n",
    "            text_with_marker = source_text[:start] + f\"[SEP] {target_word_caps} [SEP]\" + source_text[end:]\n",
    "            \n",
    "            entry = {\n",
    "                'text': text_with_marker,\n",
    "                'original_word': target_word_caps,\n",
    "                'original_text': source_text\n",
    "            }\n",
    "            \n",
    "            if is_train:\n",
    "                target_word_stressed = item['target']\n",
    "                words_source = source_text.split()\n",
    "                words_target = target_word_stressed.split()\n",
    "\n",
    "                target_token_stress = \"\"\n",
    "                for ws, wt in zip(words_source, words_target):\n",
    "                    if ws == target_word_caps:\n",
    "                        target_token_stress = wt\n",
    "                        break\n",
    "                        \n",
    "                if not target_token_stress:\n",
    "                     pattern = re.compile(target_word_caps, re.IGNORECASE)\n",
    "                     found = pattern.search(target_word_stressed)\n",
    "                     if found:\n",
    "                         target_token_stress = target_word_stressed[found.start():found.end()]\n",
    "\n",
    "                label = get_stress_label(target_word_caps, target_token_stress)\n",
    "                entry['label'] = label\n",
    "            \n",
    "            processed.append(entry)\n",
    "            \n",
    "    return processed\n",
    "\n",
    "train_data_raw = preprocess_data('data/373_homographs/train.json', is_train=True)\n",
    "test_data_raw = preprocess_data('data/373_homographs/test.json', is_train=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_data_raw)}\")\n",
    "print(f\"Test samples: {len(test_data_raw)}\")\n",
    "print(f\"Example train: {train_data_raw[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07de653c-5e58-4599-a98f-f004fa5fff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StressDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        if 'label' in item:\n",
    "            inputs['labels'] = torch.tensor(item['label'], dtype=torch.long)\n",
    "            \n",
    "        return inputs\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_split, val_split = train_test_split(train_data_raw, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = StressDataset(train_split, tokenizer, MAX_LEN)\n",
    "val_dataset = StressDataset(val_split, tokenizer, MAX_LEN)\n",
    "test_dataset = StressDataset(test_data_raw, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db64f357-4bb2-41a2-aaa0-a3be496052a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\xz1v\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc167f34e01405182ea883d86f2e87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6450, Accuracy: 0.7014\n",
      "Val Accuracy: 0.8361\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 10 \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            \n",
    "    return correct_predictions.double() / len(data_loader.dataset)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    print(f\"Train loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    val_acc = eval_model(model, val_loader)\n",
    "    print(f\"Val Accuracy: {val_acc:.4f}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bee9d3b-2785-43c1-8c6a-384a9240c3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a538dc3ea4e64c0bb7ceacb1594acf1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово! Файл data/373_homographs/submission.txt сохранен. Первые 5 предсказаний:\n",
      "['Августа', 'авгУста', 'авгУста', 'авгУста', 'адОнис']\n"
     ]
    }
   ],
   "source": [
    "def apply_stress(word, vowel_idx):\n",
    "    word_lower = word.lower()\n",
    "    vowel_positions = get_vowel_indices(word_lower)\n",
    "\n",
    "    if vowel_idx >= len(vowel_positions):\n",
    "        idx_to_stress = vowel_positions[-1]\n",
    "    else:\n",
    "        idx_to_stress = vowel_positions[vowel_idx]\n",
    "        \n",
    "    chars = list(word_lower)\n",
    "    chars[idx_to_stress] = chars[idx_to_stress].upper()\n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "predictions_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        \n",
    "        predictions_list.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "output_words = []\n",
    "for i, item in enumerate(test_data_raw):\n",
    "    word = item['original_word']\n",
    "    pred_label = predictions_list[i]\n",
    "    \n",
    "    result_word = apply_stress(word, pred_label)\n",
    "    output_words.append(result_word)\n",
    "\n",
    "output_file = 'data/373_homographs/submission.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for word in output_words:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"Готово! Файл {output_file} сохранен. Первые 5 предсказаний:\")\n",
    "print(output_words[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "069b9e0449b74c019b164a510b350acb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1574d29d48544651886727304db68f9b",
       "style": "IPY_MODEL_fae3d0486aae46698c63569cb65839f3",
       "value": "Training: 100%"
      }
     },
     "09ed27220e9a4486b4f66e3bf6fadf4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1574d29d48544651886727304db68f9b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3cc167f34e01405182ea883d86f2e87b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_069b9e0449b74c019b164a510b350acb",
        "IPY_MODEL_d96ea1f15c954a46a4ff767f8a2d72a0",
        "IPY_MODEL_78536092a30c492cb526852676b9ac60"
       ],
       "layout": "IPY_MODEL_7e558ff6bd7c4fb9abda1e95bc132720"
      }
     },
     "3fa12c23aa2544ee839cd58068234efb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4de755d8bfcc48a3971b25a39947ed60": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6da4fe9f1e2344f59cfb1c846c7c78ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "78536092a30c492cb526852676b9ac60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4de755d8bfcc48a3971b25a39947ed60",
       "style": "IPY_MODEL_c32811824fec4b188cd2bcf881b93fab",
       "value": " 1983/1983 [03:33&lt;00:00,  9.52it/s]"
      }
     },
     "7e558ff6bd7c4fb9abda1e95bc132720": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "872e61bf66db45d3a9a88203fc13b56b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e02d6c2188e84af28b61e75fafd2b310",
       "style": "IPY_MODEL_b58717d423fd46c49b184871e596f219",
       "value": "Predicting: 100%"
      }
     },
     "9940a9112b664406a45f98b64e68e264": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3fa12c23aa2544ee839cd58068234efb",
       "style": "IPY_MODEL_09ed27220e9a4486b4f66e3bf6fadf4e",
       "value": " 267/267 [00:08&lt;00:00, 32.47it/s]"
      }
     },
     "a538dc3ea4e64c0bb7ceacb1594acf1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_872e61bf66db45d3a9a88203fc13b56b",
        "IPY_MODEL_ca7a1e7426b343f7b6f038a4fe4b641f",
        "IPY_MODEL_9940a9112b664406a45f98b64e68e264"
       ],
       "layout": "IPY_MODEL_6da4fe9f1e2344f59cfb1c846c7c78ee"
      }
     },
     "b58717d423fd46c49b184871e596f219": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bd0422b785a3479ca6722d31b54815a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bf07c4fda5c74e8e8e9eb3d6ea76a581": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c32811824fec4b188cd2bcf881b93fab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c4dbf347d3db477cbca70692de167b1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ca7a1e7426b343f7b6f038a4fe4b641f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_cef7a802485949d5a404653d99bffaec",
       "max": 267,
       "style": "IPY_MODEL_c4dbf347d3db477cbca70692de167b1c",
       "value": 267
      }
     },
     "cef7a802485949d5a404653d99bffaec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d96ea1f15c954a46a4ff767f8a2d72a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_bd0422b785a3479ca6722d31b54815a7",
       "max": 1983,
       "style": "IPY_MODEL_bf07c4fda5c74e8e8e9eb3d6ea76a581",
       "value": 1983
      }
     },
     "e02d6c2188e84af28b61e75fafd2b310": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fae3d0486aae46698c63569cb65839f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
