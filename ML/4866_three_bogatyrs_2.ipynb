{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd47fef9-a94c-481f-a8d3-7e264ea4eb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|███████████████████████████████████████████████████████████| 301/301 [00:25<00:00, 11.62it/s]\n",
      "Ranking: 100%|███████████████████████████████████████████████████████████████████| 9605/9605 [00:01<00:00, 4889.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import resize, center_crop, normalize\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "model = nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "model.eval()\n",
    "\n",
    "def load_image(path):\n",
    "    img = read_image(path).float() / 255.0\n",
    "    if img.shape[0] == 1:\n",
    "        img = img.repeat(3, 1, 1)\n",
    "    img = resize(img, 256)\n",
    "    img = center_crop(img, 224)\n",
    "    img = normalize(\n",
    "        img,\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    return img\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_names):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_names = image_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return load_image(\n",
    "            os.path.join(self.image_dir, self.image_names[idx])\n",
    "        )\n",
    "\n",
    "image_dir = \"data/4866_three_bogatyrs_2/dataset/\"\n",
    "image_names = sorted(os.listdir(image_dir))\n",
    "\n",
    "dataset = ImageDataset(image_dir, image_names)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Extracting features\"):\n",
    "        batch = batch.to(device)\n",
    "        feats = model(batch)\n",
    "        feats = feats.flatten(1)\n",
    "        features.append(feats)\n",
    "\n",
    "image_features = torch.cat(features, dim=0)\n",
    "image_features = torch.nn.functional.normalize(image_features, dim=1)\n",
    "\n",
    "K = 6\n",
    "recommendations = []\n",
    "\n",
    "for i in tqdm(range(image_features.size(0)), desc=\"Ranking\"):\n",
    "    sims = image_features @ image_features[i]\n",
    "    topk = torch.topk(sims, K + 1).indices[1:]\n",
    "    recommendations.append(topk.cpu().tolist())\n",
    "\n",
    "rows = []\n",
    "for i, name in enumerate(image_names):\n",
    "    rows.append([\n",
    "        name,\n",
    "        \" \".join(image_names[j] for j in recommendations[i])\n",
    "    ])\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"filename\", \"ranking\"])\n",
    "df.to_csv(\n",
    "    \"data/4866_three_bogatyrs_2/submission.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ae4821-141d-4ebc-9a5d-68096554e58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf031cd513f41a69ad3fd6442286216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xz1v\\AppData\\Roaming\\Python\\Python311\\site-packages\\open_clip\\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n",
      "Extracting CLIP features: 100%|██████████████████████████████████████████████████████| 151/151 [00:18<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP + FAISS done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import resize, center_crop\n",
    "import open_clip\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-L-14\",\n",
    "    pretrained=\"openai\"\n",
    ")\n",
    "model = model.to(device).half()\n",
    "model.eval()\n",
    "\n",
    "def load_image(path):\n",
    "    img = read_image(path).float() / 255.0\n",
    "    if img.shape[0] == 1:\n",
    "        img = img.repeat(3, 1, 1)\n",
    "    img = resize(img, 224)\n",
    "    img = center_crop(img, 224)\n",
    "    return img\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_names):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_names = image_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return load_image(\n",
    "            os.path.join(self.image_dir, self.image_names[idx])\n",
    "        )\n",
    "\n",
    "image_dir = \"data/4866_three_bogatyrs_2/dataset/\"\n",
    "image_names = sorted(os.listdir(image_dir))\n",
    "\n",
    "dataset = ImageDataset(image_dir, image_names)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Extracting CLIP features\"):\n",
    "        batch = batch.to(device).half()\n",
    "        feats = model.encode_image(batch)\n",
    "        feats = torch.nn.functional.normalize(feats, dim=1)\n",
    "        features.append(feats)\n",
    "\n",
    "features = torch.cat(features, dim=0).cpu().numpy().astype(\"float32\")\n",
    "\n",
    "index = faiss.IndexFlatIP(features.shape[1])\n",
    "index.add(features)\n",
    "\n",
    "K = 6\n",
    "_, indices = index.search(features, K + 1)\n",
    "\n",
    "rows = []\n",
    "for i, name in enumerate(image_names):\n",
    "    rows.append([\n",
    "        name,\n",
    "        \" \".join(image_names[j] for j in indices[i][1:])\n",
    "    ])\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"filename\", \"ranking\"])\n",
    "df.to_csv(\"data/4866_three_bogatyrs_2/submission_clip.csv\", index=False)\n",
    "\n",
    "print(\"CLIP + FAISS done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "371b9e019cea4886a2f37ebf2446ac64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "420f51bbed2142c780bb1061cfe877fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4cf031cd513f41a69ad3fd6442286216": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_6b76f4adf1a848478cb984fffa75bafb",
        "IPY_MODEL_56ad73749b694db59e60c7dd88ea6cb8",
        "IPY_MODEL_aa3b06bde20b42ff80da143523135777"
       ],
       "layout": "IPY_MODEL_54c5a099b3ed408e958d01dc1fa10d3b"
      }
     },
     "54c5a099b3ed408e958d01dc1fa10d3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "56ad73749b694db59e60c7dd88ea6cb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_8125c0f7c8894577976b8a489ade771c",
       "max": 1710517724,
       "style": "IPY_MODEL_ef78d07484f14052aad6c4b2842ab4a9",
       "value": 1710517724
      }
     },
     "6b76f4adf1a848478cb984fffa75bafb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6b89ffd0b9ee4a9f993a17ab7ffda84a",
       "style": "IPY_MODEL_371b9e019cea4886a2f37ebf2446ac64",
       "value": "open_clip_model.safetensors: 100%"
      }
     },
     "6b89ffd0b9ee4a9f993a17ab7ffda84a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6f7f18f49f304f5891aaa7330bfa377f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8125c0f7c8894577976b8a489ade771c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aa3b06bde20b42ff80da143523135777": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_420f51bbed2142c780bb1061cfe877fb",
       "style": "IPY_MODEL_6f7f18f49f304f5891aaa7330bfa377f",
       "value": " 1.71G/1.71G [01:10&lt;00:00, 24.9MB/s]"
      }
     },
     "ef78d07484f14052aad6c4b2842ab4a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
